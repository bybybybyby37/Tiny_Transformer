{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be51c0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nimport torchvision\\nprint(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\\nif not torch.__version__.startswith(\"2.2\"):\\n    print(\"you are using an another version of PyTorch. We expect PyTorch 2.2. You may continue using your version but it\"\\n          \" might cause dependency and compatibility issues.\")\\nif not torchvision.__version__.startswith(\"0.17\"):\\n    print(\"you are using an another version of torchvision. We expect torchvision 0.17. You can continue with your version but it\"\\n          \" might cause dependency and compatibility issues.\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install Pytorch and tensorboard\n",
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch==2.2.2+cu118 torchvision==0.17.2+cu118 torchaudio==2.2.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "!{sys.executable} -m pip install tensorboard==2.9.1\n",
    "'''\n",
    "#Install torch and torchvision\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"2.2\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 2.2. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.17\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.17. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5408cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important imports\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import tensorboard\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import TokenAndPositionEmbedding\n",
    "from models.transformer import TransformerBlock\n",
    "\n",
    "from models import tokenizer, embedding, transformer\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2976c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "data_path     = \"data/tiny_shakespeare.txt\"\n",
    "save_path = \"output/best.pt\"\n",
    "split_ratio   = (0.6, 0.2, 0.2)   # train/val/test\n",
    "block_size    = 256\n",
    "batch_size    = 32\n",
    "#vocab_size = 65 \n",
    "patience  = 5\n",
    "\n",
    "d_model       = 256\n",
    "n_heads       = 8\n",
    "n_layers      = 6\n",
    "d_ff          = 4 * d_model\n",
    "dropout       = 0.1\n",
    "\n",
    "learning_rate = 0.002\n",
    "weight_decay  = 0.01\n",
    "grad_clip     = 1.0\n",
    "max_iters     = 5000\n",
    "eval_interval = 100\n",
    "eval_iters    = 100\n",
    "seed          = 1337\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16f8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/full_shakespeare.txt' already exists, skipping download.\n",
      "100 unique chars\n",
      "vocab_size = 100\n",
      "Total tokens: 5,359,388\n",
      "Train: 3,215,632, Val: 1,071,877, Test: 1,071,879\n",
      "torch.Size([32, 256]) torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Download dataset tiny shakespeare\n",
    "'''data_path = \"data/tiny_shakespeare.txt\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"'{data_path}' already exists, skipping download.\")\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    with open(\"data/tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(\"Tiny Shakespeare downloaded! File size:\", len(text), \"characters\")'''\n",
    "\n",
    "#Download dataset full shakespeare\n",
    "data_path = \"data/full_shakespeare.txt\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"'{data_path}' already exists, skipping download.\")\n",
    "else:\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "    print(\"Downloading full Shakespeare from Project Gutenberg...\")\n",
    "    text = requests.get(url).text\n",
    "\n",
    "    # Keep only the text\n",
    "    if \"*** START\" in text:\n",
    "        text = text.split(\"*** START\")[1]\n",
    "    if \"*** END\" in text:\n",
    "        text = text.split(\"*** END\")[0]\n",
    "\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(\"Full Shakespeare downloaded! File size:\", len(text), \"characters\")\n",
    "\n",
    "#Tokenizer\n",
    "'''with open(\"data/tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()'''\n",
    "\n",
    "with open(\"data/full_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tok = tokenizer.CharTokenizer(text)\n",
    "print(len(tok.chars), \"unique chars\")\n",
    "\n",
    "ids = tok.encode(text)\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "#Calculate vocab_size using the actual character set of the tokenizer\n",
    "vocab_size = getattr(tok, \"vocab_size\", len(tok.chars))\n",
    "print(\"vocab_size =\", vocab_size)\n",
    "mx = int(max(ids)) if len(ids) > 0 else -1\n",
    "assert mx < int(vocab_size), f\"max id {mx} >= vocab_size {int(vocab_size)}\"\n",
    "\n",
    "# Split by split_ratio (can be adjusted in hyperparameter tuning)\n",
    "n = len(data)\n",
    "n_train = int(split_ratio[0] * n)\n",
    "n_val = int(split_ratio[1] * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:n_train + n_val]\n",
    "test_data = data[n_train + n_val:]\n",
    "\n",
    "print(f\"Total tokens: {n:,}\")\n",
    "print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}, Test: {len(test_data):,}\")\n",
    "\n",
    "\n",
    "#Function for get batch\n",
    "def get_batch(split):\n",
    "    data_split = {\"train\": train_data, \"val\": val_data, \"test\": test_data}[split]\n",
    "    ix = torch.randint(0, len(data_split) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data_split[i     : i + block_size]     for i in ix])\n",
    "    y = torch.stack([data_split[i + 1 : i + 1 + block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# quick shape check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)  # Expect: (batch_size, block_size)\n",
    "\n",
    "\n",
    "#define module\n",
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionEmbedding(vocab_size, d_model, block_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout, block_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embed(idx)                      # (B,T,C)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                    # (B,T,V)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abc3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 4855908\n"
     ]
    }
   ],
   "source": [
    "#set up a new model\n",
    "model = MiniTransformerLM().to(device)\n",
    "print(\"Params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82131ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logdir: runs/tt_20251018-175737\n",
      "step    1 | train_loss 4.8431 | val_loss 3.7555\n",
      "improved! best_val=3.7555 (saved)\n",
      "step  100 | train_loss 2.4878 | val_loss 2.5030\n",
      "improved! best_val=2.5030 (saved)\n",
      "step  200 | train_loss 2.3784 | val_loss 2.3183\n",
      "improved! best_val=2.3183 (saved)\n",
      "step  300 | train_loss 1.9972 | val_loss 2.0274\n",
      "improved! best_val=2.0274 (saved)\n",
      "step  400 | train_loss 1.8852 | val_loss 1.8750\n",
      "improved! best_val=1.8750 (saved)\n",
      "step  500 | train_loss 1.7375 | val_loss 1.7815\n",
      "improved! best_val=1.7815 (saved)\n",
      "step  600 | train_loss 1.6813 | val_loss 1.7171\n",
      "improved! best_val=1.7171 (saved)\n",
      "step  700 | train_loss 1.6112 | val_loss 1.6739\n",
      "improved! best_val=1.6739 (saved)\n",
      "step  800 | train_loss 1.5338 | val_loss 1.6454\n",
      "improved! best_val=1.6454 (saved)\n",
      "step  900 | train_loss 1.5259 | val_loss 1.6096\n",
      "improved! best_val=1.6096 (saved)\n",
      "step 1000 | train_loss 1.5166 | val_loss 1.5913\n",
      "improved! best_val=1.5913 (saved)\n",
      "step 1100 | train_loss 1.4551 | val_loss 1.5777\n",
      "improved! best_val=1.5777 (saved)\n",
      "step 1200 | train_loss 1.5017 | val_loss 1.5453\n",
      "improved! best_val=1.5453 (saved)\n",
      "step 1300 | train_loss 1.4379 | val_loss 1.5447\n",
      "improved! best_val=1.5447 (saved)\n",
      "step 1400 | train_loss 1.3761 | val_loss 1.5240\n",
      "improved! best_val=1.5240 (saved)\n",
      "step 1500 | train_loss 1.3540 | val_loss 1.5196\n",
      "improved! best_val=1.5196 (saved)\n",
      "step 1600 | train_loss 1.4389 | val_loss 1.5064\n",
      "improved! best_val=1.5064 (saved)\n",
      "step 1700 | train_loss 1.4183 | val_loss 1.5025\n",
      "improved! best_val=1.5025 (saved)\n",
      "step 1800 | train_loss 1.3810 | val_loss 1.4962\n",
      "improved! best_val=1.4962 (saved)\n",
      "step 1900 | train_loss 1.3991 | val_loss 1.4884\n",
      "improved! best_val=1.4884 (saved)\n",
      "step 2000 | train_loss 1.3539 | val_loss 1.4886\n",
      "no improvement (1/5)\n",
      "step 2100 | train_loss 1.3554 | val_loss 1.4772\n",
      "improved! best_val=1.4772 (saved)\n",
      "step 2200 | train_loss 1.3638 | val_loss 1.4817\n",
      "no improvement (1/5)\n",
      "step 2300 | train_loss 1.3123 | val_loss 1.4705\n",
      "improved! best_val=1.4705 (saved)\n",
      "step 2400 | train_loss 1.3013 | val_loss 1.4642\n",
      "improved! best_val=1.4642 (saved)\n",
      "step 2500 | train_loss 1.3217 | val_loss 1.4579\n",
      "improved! best_val=1.4579 (saved)\n",
      "step 2600 | train_loss 1.3661 | val_loss 1.4598\n",
      "no improvement (1/5)\n",
      "step 2700 | train_loss 1.3172 | val_loss 1.4581\n",
      "no improvement (2/5)\n",
      "step 2800 | train_loss 1.3236 | val_loss 1.4542\n",
      "improved! best_val=1.4542 (saved)\n",
      "step 2900 | train_loss 1.2615 | val_loss 1.4503\n",
      "improved! best_val=1.4503 (saved)\n",
      "step 3000 | train_loss 1.3260 | val_loss 1.4425\n",
      "improved! best_val=1.4425 (saved)\n",
      "step 3100 | train_loss 1.2710 | val_loss 1.4383\n",
      "improved! best_val=1.4383 (saved)\n",
      "step 3200 | train_loss 1.2983 | val_loss 1.4390\n",
      "no improvement (1/5)\n",
      "step 3300 | train_loss 1.2395 | val_loss 1.4411\n",
      "no improvement (2/5)\n",
      "step 3400 | train_loss 1.2829 | val_loss 1.4451\n",
      "no improvement (3/5)\n",
      "step 3500 | train_loss 1.2585 | val_loss 1.4313\n",
      "improved! best_val=1.4313 (saved)\n",
      "step 3600 | train_loss 1.3116 | val_loss 1.4290\n",
      "improved! best_val=1.4290 (saved)\n",
      "step 3700 | train_loss 1.2550 | val_loss 1.4274\n",
      "improved! best_val=1.4274 (saved)\n",
      "step 3800 | train_loss 1.2102 | val_loss 1.4235\n",
      "improved! best_val=1.4235 (saved)\n",
      "step 3900 | train_loss 1.2760 | val_loss 1.4218\n",
      "improved! best_val=1.4218 (saved)\n",
      "step 4000 | train_loss 1.2494 | val_loss 1.4162\n",
      "improved! best_val=1.4162 (saved)\n",
      "step 4100 | train_loss 1.2496 | val_loss 1.4167\n",
      "no improvement (1/5)\n",
      "step 4200 | train_loss 1.2595 | val_loss 1.4126\n",
      "improved! best_val=1.4126 (saved)\n",
      "step 4300 | train_loss 1.3229 | val_loss 1.4167\n",
      "no improvement (1/5)\n",
      "step 4400 | train_loss 1.2311 | val_loss 1.4118\n",
      "improved! best_val=1.4118 (saved)\n",
      "step 4500 | train_loss 1.2766 | val_loss 1.4083\n",
      "improved! best_val=1.4083 (saved)\n",
      "step 4600 | train_loss 1.2042 | val_loss 1.4146\n",
      "no improvement (1/5)\n",
      "step 4700 | train_loss 1.2537 | val_loss 1.4095\n",
      "no improvement (2/5)\n",
      "step 4800 | train_loss 1.1750 | val_loss 1.4044\n",
      "improved! best_val=1.4044 (saved)\n",
      "step 4900 | train_loss 1.1878 | val_loss 1.3988\n",
      "improved! best_val=1.3988 (saved)\n",
      "step 5000 | train_loss 1.2139 | val_loss 1.3962\n",
      "improved! best_val=1.3962 (saved)\n",
      "Final Test Loss: 1.4185\n",
      "Perplexity (PPL): 4.13\n"
     ]
    }
   ],
   "source": [
    "#Tensorboard setup\n",
    "run_dir = f\"runs/tt_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(\"TensorBoard logdir:\", run_dir)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "\n",
    "#Start training process\n",
    "best_val   = float(\"inf\")\n",
    "bad_epochs = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(split):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(eval_iters):\n",
    "        xb, yb = get_batch(split)\n",
    "        _, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "for step in range(1, max_iters + 1):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss for each step\n",
    "    writer.add_scalar(\"loss/train\", loss.item(), step)\n",
    "\n",
    "    if step % eval_interval == 0 or step == 1:\n",
    "        val_loss = estimate_loss(\"val\")\n",
    "        print(f\"step {step:4d} | train_loss {loss.item():.4f} | val_loss {val_loss:.4f}\")\n",
    "\n",
    "        # Record loss and learning rate\n",
    "        writer.add_scalar(\"loss/val\", val_loss, step)\n",
    "        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], step)\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            bad_epochs = 0\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "                \"best_val\": best_val,\n",
    "            }, save_path)\n",
    "            print(f\"improved! best_val={best_val:.4f} (saved)\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            print(f\"no improvement ({bad_epochs}/{patience})\")\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "\n",
    "#test result best model\n",
    "@torch.no_grad()\n",
    "def evaluate_test_set():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(eval_iters):\n",
    "        xb, yb = get_batch(\"test\")\n",
    "        _, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    return test_loss\n",
    "\n",
    "test_loss = evaluate_test_set()\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {torch.exp(torch.tensor(test_loss)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8431f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: ’tis at enemies.\n",
      "\n",
      "POMPEY.\n",
      "If it so, all proud as far you, Clifford as they say at a\n",
      "state heart, the clome reckon hath his change.\n",
      "\n",
      "OPHELIA.\n",
      "Peace, sir, had he;\n",
      "All hoisted\n",
      "More of your discharge your lute bones upon you.\n",
      "\n",
      "AUDREY.\n",
      "Sir, not begget him; Clifford, from his sons!\n",
      "\n",
      "ALEXAS.\n",
      "And fetch the mellaries of his scroll brave\n",
      "Have been arged hereafter of sacrifice,\n",
      "Nor come than makes her merch\n"
     ]
    }
   ],
   "source": [
    "#test sample output text generation\n",
    "model.eval()\n",
    "# Test output: Generate from \"ROMEO:\"\n",
    "start_ids = tok.encode(\"ROMEO:\")\n",
    "idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
    "out = model.generate(idx, max_new_tokens=400, temperature=0.9, top_k=50)\n",
    "print(tok.decode(out[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
