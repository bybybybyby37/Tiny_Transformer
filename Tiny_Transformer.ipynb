{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be51c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Pytorch\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install torch==2.2.2+cu118 torchvision==0.17.2+cu118 torchaudio==2.2.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "#!{sys.executable} -m pip install tensorboard==2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0a470b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nimport torchvision\\nprint(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\\nif not torch.__version__.startswith(\"2.2\"):\\n    print(\"you are using an another version of PyTorch. We expect PyTorch 2.2. You may continue using your version but it\"\\n          \" might cause dependency and compatibility issues.\")\\nif not torchvision.__version__.startswith(\"0.17\"):\\n    print(\"you are using an another version of torchvision. We expect torchvision 0.17. You can continue with your version but it\"\\n          \" might cause dependency and compatibility issues.\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"2.2\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 2.2. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.17\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.17. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5408cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import tensorboard\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import TokenAndPositionEmbedding\n",
    "from models.transformer import TransformerBlock\n",
    "\n",
    "from models import tokenizer, embedding, transformer\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16f8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/tiny_shakespeare.txt' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/tiny_shakespeare.txt\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"'{data_path}' already exists, skipping download.\")\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    with open(\"data/tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(\"Tiny Shakespeare downloaded! File size:\", len(text), \"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23206f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique chars\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "with open(\"data/tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tok = tokenizer.CharTokenizer(text)\n",
    "print(len(tok.chars), \"unique chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0988280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1,115,394\n",
      "Train: 669,236, Val: 223,078, Test: 223,080\n"
     ]
    }
   ],
   "source": [
    "ids = tok.encode(text)\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# Split by 6:2:2\n",
    "n = len(data)\n",
    "n_train = int(0.6 * n)\n",
    "n_val = int(0.2 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:n_train + n_val]\n",
    "test_data = data[n_train + n_val:]\n",
    "\n",
    "print(f\"Total tokens: {n:,}\")\n",
    "print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}, Test: {len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f60694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab_size = 65 \\nd_model = 128\\nblock_size = 256\\n\\nembed = embedding.TokenAndPositionEmbedding(vocab_size, d_model, block_size)\\n\\nx = torch.randint(0, vocab_size, (4, 20))\\n\\nout = embed(x)\\n\\nprint(\"Input shape:\", x.shape)\\nprint(\"Output shape:\", out.shape)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''vocab_size = 65 \n",
    "d_model = 128\n",
    "block_size = 256\n",
    "\n",
    "embed = embedding.TokenAndPositionEmbedding(vocab_size, d_model, block_size)\n",
    "\n",
    "x = torch.randint(0, vocab_size, (4, 20))\n",
    "\n",
    "out = embed(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625b6a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.manual_seed(0)\\nB, T, C = 2, 16, 128\\nH, D = 4, C // 4\\nblock_size = 32\\ndropout = 0.1\\n\\nx = torch.randn(B, T, C)\\n\\n# Only Attention\\nattn = transformer.CausalSelfAttention(d_model=C, n_heads=H, dropout=dropout, block_size=block_size)\\ny = attn(x)  # (B, T, C)\\nprint(\"attn out:\", y.shape)\\n\\n# Full Block\\nblock = transformer.TransformerBlock(d_model=C, n_heads=H, d_ff=4*C, dropout=dropout, block_size=block_size)\\nz = block(x)\\nprint(\"block out:\", z.shape)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''torch.manual_seed(0)\n",
    "B, T, C = 2, 16, 128\n",
    "H, D = 4, C // 4\n",
    "block_size = 32\n",
    "dropout = 0.1\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Only Attention\n",
    "attn = transformer.CausalSelfAttention(d_model=C, n_heads=H, dropout=dropout, block_size=block_size)\n",
    "y = attn(x)  # (B, T, C)\n",
    "print(\"attn out:\", y.shape)\n",
    "\n",
    "# Full Block\n",
    "block = transformer.TransformerBlock(d_model=C, n_heads=H, d_ff=4*C, dropout=dropout, block_size=block_size)\n",
    "z = block(x)\n",
    "print(\"block out:\", z.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e4aa9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "data_path     = \"data/tiny_shakespeare.txt\"\n",
    "save_path = \"output/best.pt\"\n",
    "split_ratio   = (0.6, 0.2, 0.2)   # train/val/test\n",
    "block_size    = 256\n",
    "batch_size    = 32\n",
    "vocab_size = 65 \n",
    "patience  = 5\n",
    "\n",
    "d_model       = 256\n",
    "n_heads       = 8\n",
    "n_layers      = 6\n",
    "d_ff          = 4 * d_model\n",
    "dropout       = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay  = 0.01\n",
    "grad_clip     = 1.0\n",
    "max_iters     = 2000\n",
    "eval_interval = 200\n",
    "eval_iters    = 100\n",
    "seed          = 1337\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def seed_everything(seed=1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98524dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256]) torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data_split = {\"train\": train_data, \"val\": val_data, \"test\": test_data}[split]\n",
    "    ix = torch.randint(0, len(data_split) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data_split[i     : i + block_size]     for i in ix])\n",
    "    y = torch.stack([data_split[i + 1 : i + 1 + block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# quick shape check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)  # Expect: (batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abc3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 4837953\n"
     ]
    }
   ],
   "source": [
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionEmbedding(vocab_size, d_model, block_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout, block_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embed(idx)                      # (B,T,C)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                    # (B,T,V)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "model = MiniTransformerLM().to(device)\n",
    "print(\"Params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb0dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logdir: runs/tt_20251015-112426\n"
     ]
    }
   ],
   "source": [
    "run_dir = f\"runs/tt_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(\"TensorBoard logdir:\", run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82131ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 | train_loss 4.3388 | val_loss 3.4862\n",
      "improved! best_val=3.4862 (saved)\n",
      "step  200 | train_loss 2.3123 | val_loss 2.3620\n",
      "improved! best_val=2.3620 (saved)\n",
      "step  400 | train_loss 1.8553 | val_loss 1.9898\n",
      "improved! best_val=1.9898 (saved)\n",
      "step  600 | train_loss 1.6159 | val_loss 1.8181\n",
      "improved! best_val=1.8181 (saved)\n",
      "step  800 | train_loss 1.5638 | val_loss 1.7583\n",
      "improved! best_val=1.7583 (saved)\n",
      "step 1000 | train_loss 1.4131 | val_loss 1.6956\n",
      "improved! best_val=1.6956 (saved)\n",
      "step 1200 | train_loss 1.4078 | val_loss 1.6669\n",
      "improved! best_val=1.6669 (saved)\n",
      "step 1400 | train_loss 1.3200 | val_loss 1.6458\n",
      "improved! best_val=1.6458 (saved)\n",
      "step 1600 | train_loss 1.3025 | val_loss 1.6346\n",
      "improved! best_val=1.6346 (saved)\n",
      "step 1800 | train_loss 1.2777 | val_loss 1.6171\n",
      "improved! best_val=1.6171 (saved)\n",
      "step 2000 | train_loss 1.2480 | val_loss 1.6245\n",
      "no improvement (1/5)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "best_val   = float(\"inf\")\n",
    "bad_epochs = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(split):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(eval_iters):\n",
    "        xb, yb = get_batch(split)\n",
    "        _, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "for step in range(1, max_iters + 1):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss for each step\n",
    "    writer.add_scalar(\"loss/train\", loss.item(), step)\n",
    "\n",
    "    if step % eval_interval == 0 or step == 1:\n",
    "        val_loss = estimate_loss(\"val\")\n",
    "        print(f\"step {step:4d} | train_loss {loss.item():.4f} | val_loss {val_loss:.4f}\")\n",
    "\n",
    "        # Record loss and learning rate\n",
    "        writer.add_scalar(\"loss/val\", val_loss, step)\n",
    "        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], step)\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            bad_epochs = 0\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "                \"best_val\": best_val,\n",
    "            }, save_path)\n",
    "            print(f\"improved! best_val={best_val:.4f} (saved)\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            print(f\"no improvement ({bad_epochs}/{patience})\")\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8431f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The king should proceed follow me provese.\n",
      "Both one ye, my gracious lord, Buckingham.\n",
      "The fieldess of the mark, and there, as they will love,\n",
      "In leave her answer love to say a mercy;\n",
      "Why, art thou art lark'st done summlant,\n",
      "And be seem'd by this other ten mornings aid:\n",
      "The other steely so fond account of side as deputate,\n",
      "Tidings hast thou along upon Lancaster,\n",
      "Whose wickets and still not long an\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Test output: Generate from \"ROMEO:\"\n",
    "start_ids = tok.encode(\"ROMEO:\")\n",
    "idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
    "out = model.generate(idx, max_new_tokens=400, temperature=0.9, top_k=50)\n",
    "print(tok.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff4c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 1.7903\n",
      "Perplexity (PPL): 5.99\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test_set():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(eval_iters):\n",
    "        xb, yb = get_batch(\"test\")\n",
    "        _, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    return test_loss\n",
    "\n",
    "test_loss = evaluate_test_set()\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {torch.exp(torch.tensor(test_loss)):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
